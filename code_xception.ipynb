{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e6e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1066162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c26990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aace323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, ZeroPadding3D , BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7880b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ed9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "img_height = 200\n",
    "img_width = 200 \n",
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda5c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir = r\"C:\\Users\\kuric\\Desktop\\AI Camp\\Code\\Project_back\\Data_aug\"\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "023f6b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n"
     ]
    }
   ],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "756e41fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8316 files belonging to 6 classes.\n",
      "Using 2495 files for training.\n",
      "Found 8316 files belonging to 6 classes.\n",
      "Using 2494 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.7,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "477721d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bungarus', 'Calloselasma', 'Daboia_russelii', 'Naja', 'Ophiophagus', 'Trimeresurus']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0621481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_ds.take(1):\n",
    "#     for i in range(6):\n",
    "#         ax = plt.subplot(4, 4, i + 1)\n",
    "#         plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#         plt.title(class_names[labels[i]])\n",
    "#         plt.axis(\"off\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea0c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import xception\n",
    "from keras.layers import GlobalAveragePooling2D, Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.models import Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "modelName = \"Xception\"\n",
    "\n",
    "base_model = xception.Xception(include_top=False,\n",
    "                    input_shape=(img_height,img_width,3),\n",
    "                    pooling='avg',classes=NUM_CLASSES,\n",
    "                    weights='imagenet')\n",
    "\n",
    "for layer in base_model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "\n",
    "################################################\n",
    "nb_epochs=20\n",
    "################################################\n",
    "\n",
    "learningRate = 0.001\n",
    "loss_function = \"categorical_crossentropy\"\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_function,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a61f4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " xception (Functional)       (None, 2048)              20861480  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,913,646\n",
      "Trainable params: 1,052,166\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f6e9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Train  with backup h5 every epoch. #####################\n",
    "\n",
    "\n",
    "# checkpoint_path = r\"./ModelSaving/\"+modelName+\"-{epoch:04d}.h5\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     checkpoint_path, verbose=1, save_weights_only=False,\n",
    "#     # Save weights, every epoch.\n",
    "#     save_freq='epoch'\n",
    "# )\n",
    "# resnet_model.save(checkpoint_path.format(epoch=0))\n",
    "# vg = validation_generator\n",
    "# history = resnet_model.fit( train_generator, validation_data=vg ,epochs=nb_epochs, callbacks = [cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e5ff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "156/156 [==============================] - 35s 168ms/step - loss: 17.7846 - accuracy: 0.2337 - val_loss: 6.3467 - val_accuracy: 0.3128\n",
      "Epoch 2/50\n",
      "156/156 [==============================] - 30s 193ms/step - loss: 5.6391 - accuracy: 0.3158 - val_loss: 5.8084 - val_accuracy: 0.2815\n",
      "Epoch 3/50\n",
      "155/156 [============================>.] - ETA: 0s - loss: 3.9363 - accuracy: 0.3250"
     ]
    }
   ],
   "source": [
    "history = model.fit( train_ds, validation_data=val_ds ,epochs=nb_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e595241",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(nb_epochs)\n",
    "\n",
    "print(\"Model = \"+model_name)\n",
    "print(\"Epochs = {}\".format(nb_epochs))\n",
    "print(\"Image Size = {}\".format(img_width))\n",
    "print(\"Batch = {}\".format(batch_size))\n",
    "print(\"learningRate = {}\".format(learningRate))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "#####  use this  to save train graph as png #####################\n",
    "from datetime import datetime\n",
    "dt = datetime.now()\n",
    "ts = datetime.timestamp(dt)\n",
    "date_time = datetime.fromtimestamp(ts)\n",
    "str_date_time = date_time.strftime(\"%d-%m-%Y_%H-%M\")\n",
    "graph_path = r'./experimentData/graph'\n",
    "if( not (os.path.exists(graph_path)) ) :\n",
    "    os.mkdir(graph_path)\n",
    "plt.savefig(graph_path+'/'+str_date_time+\".png\")  \n",
    "#####  use this  to save train graph as png #####################\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r\"./ModelSaving/FinalModel_\"+modelName+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "dt = datetime.now()\n",
    "ts = datetime.timestamp(dt)\n",
    "date_time = datetime.fromtimestamp(ts)\n",
    "str_date_time = date_time.strftime(\"%d/%m/%Y, %H:%M\")\n",
    "\n",
    "\n",
    "\n",
    "############################# List that var should be defined   \n",
    "############################# รายชื่อตัวแปร ที่ควรสร้าง และใส่ค่า \n",
    "varList = {\"model_name\",   ### name of model\n",
    "           \"nb_epochs\",     ### number of epochs\n",
    "           \"batch_size\",    ###  batch size\n",
    "           \"learningRate\",  ### learning rate\n",
    "           \"accuracy\",       ###  accuracy of model\n",
    "           \"val_accuracy\",  ### val_accuracy  of model\n",
    "           \"loss\",          ### loss  of model\n",
    "           \"val_loss\",      ### val_loss  of model\n",
    "           \"loss_function\", ### lossfunction name    Must be str\n",
    "           \"graph_path\",    ###  path to graph dir\n",
    "           \"exp_dataset\",  ###  explain about dataset\n",
    "           \"other_comment\"}  ###  option  for comment\n",
    "#############################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pathDir = r'./experimentData'\n",
    "path = r'./experimentData/Experiment.csv'\n",
    "\n",
    "ExpDict = {\n",
    "            'Date': [],\n",
    "            'ModelName': [],\n",
    "            'Epochs' : [],\n",
    "            'ImageSize' : [],\n",
    "            'Batch': [],\n",
    "            'LearningRate' : [],\n",
    "            'Accuracy' : [],\n",
    "            'valAccuracy': [],\n",
    "            'loss' : [],\n",
    "            'valLoss' : [],\n",
    "            'lossFunction' : [],\n",
    "            'GraphPath' : [],\n",
    "            'ExpDataSet' : [],\n",
    "            'otherComment' : []\n",
    "             }\n",
    "\n",
    "if(not (os.path.exists(path))) :\n",
    "    dt = pd.DataFrame(ExpDict)\n",
    "    if( not (os.path.exists(pathDir)) ) :\n",
    "            os.mkdir(pathDir)\n",
    "    dt.to_csv(path,na_rep='',float_format='%.2f',index=False)\n",
    "\n",
    "expExel = pd.read_csv(path)\n",
    "\n",
    "for var in varList :\n",
    "    if not(var in globals()):\n",
    "        exec(var + \" = ''\")\n",
    "\n",
    "        \n",
    "ExpDict[\"Date\"] = str_date_time\n",
    "ExpDict[\"ModelName\"] = model_name\n",
    "ExpDict[\"Epochs\"] = str(nb_epochs)\n",
    "if (\"img_height\" in globals()) and  (img_width in globals()):\n",
    "    ExpDict[\"ImageSize\"] = str(\"HeightWidth = {}*{}\".format(img_height,img_width))\n",
    "elif (\"img_size\" in globals()):\n",
    "    ExpDict[\"ImageSize\"] = str(\"ImageSize = {}\".format(img_size))\n",
    "else :\n",
    "    ExpDict[\"ImageSize\"] = \"\"\n",
    "ExpDict[\"Batch\"] = str(batch_size)\n",
    "ExpDict[\"LearningRate\"] = str(learningRate)\n",
    "ExpDict[\"Accuracy\"] = str(accuracy)\n",
    "ExpDict[\"valAccuracy\"] = str(val_accuracy)\n",
    "ExpDict[\"loss\"] = str(loss)\n",
    "ExpDict[\"valLoss\"] = str(val_loss)\n",
    "ExpDict[\"lossFunction\"] = loss_function\n",
    "ExpDict[\"GraphPath\"] = graph_path\n",
    "ExpDict[\"ExpDataSet\"] = exp_dataset\n",
    "ExpDict[\"otherComment\"] = other_comment\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e67e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  บันทึกทับลง exel\n",
    "expExel = expExel.append(ExpDict,ignore_index=True)\n",
    "expExel.to_csv(path,na_rep='',float_format='%.2f',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scAI]",
   "language": "python",
   "name": "conda-env-scAI-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
